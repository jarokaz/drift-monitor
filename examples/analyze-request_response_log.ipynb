{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing AI Platform Prediction Request Response Log using TensorFlow Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "from tensorflow_data_validation.utils.stats_util import get_feature_stats\n",
    "from tensorflow_data_validation.utils import slicing_util\n",
    "from tensorflow_data_validation import FeaturePath\n",
    "\n",
    "from typing import List, Optional, Text, Union, Dict, Iterable, Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'mlops-dev-env'\n",
    "response_request_log_table = 'data_validation.covertype_classifier_logs_tf'\n",
    "local_workspace = '/home/jarekk/workspace/analysis'\n",
    "local_tfrecords_file = '{}/log_records.tfrecords'.format(local_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Removing previous workspace artifacts...\nCreating a new workspace...\n"
    }
   ],
   "source": [
    "if tf.io.gfile.exists(local_workspace):\n",
    "  print(\"Removing previous workspace artifacts...\")\n",
    "  tf.io.gfile.rmtree(local_workspace)\n",
    "\n",
    "print(\"Creating a new workspace...\")\n",
    "tf.io.gfile.makedirs(local_workspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve log records from BigQuey and convert them to a TFRecords file\n",
    "\n",
    "Although TFDV provides a function to calculate statistics on a Pandas dataframe, the function does not support slicing. To mitigate, we will convert the log records to TFRecords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and run a sampling query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(table_name, start_time, end_time):\n",
    "\n",
    "  sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}`\n",
    "       WHERE time BETWEEN '{{ start_time }}' AND '{{ end_time }}'\n",
    "       \"\"\"\n",
    "  \n",
    "  query = Template(sampling_query_template).render(\n",
    "      source_table=table_name, start_time=start_time, end_time=end_time)\n",
    "\n",
    "  return query\n",
    "\n",
    "start_time = '2020-05-14T00:00:00'\n",
    "end_time = '2020-05-21T00:00:00'\n",
    "\n",
    "query = generate_query(response_request_log_table, start_time, end_time)\n",
    "\n",
    "client = bigquery.Client()\n",
    "query_job = client.query(query)\n",
    "rows = query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the results to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature_dict: Dict) -> tf.train.Example:\n",
    "  example = tf.train.Example()\n",
    "\n",
    "  for name, values in feature_dict.items():\n",
    "    feature = example.features.feature[name]\n",
    "    if isinstance(values[0], str):\n",
    "      values = [value.encode() for value in values]\n",
    "      add = feature.bytes_list.value.extend\n",
    "    elif isinstance(values[0], float):\n",
    "      add = feature.float32_list.value.extend\n",
    "    elif isinstance(values[0], int):\n",
    "      add = feature.int64_list.value.extend\n",
    "    else:\n",
    "      raise AssertionError('Unsupported type: %s' % type(values[0]))\n",
    "    add(np.array(values))\n",
    "\n",
    "  return example.SerializeToString()\n",
    "\n",
    "\n",
    "with tf.io.TFRecordWriter(local_tfrecords_file) as tfrecord_writer:\n",
    "    for row in rows:\n",
    "        raw_data = json.loads(row['raw_data'])\n",
    "        time_stamp = row['time'].date().isoformat()\n",
    "        for instance in raw_data['instances']:\n",
    "            feature_dict = ({column_name: value\n",
    "                for column_name, value in instance.items()})\n",
    "            feature_dict['time_window'] = [time_stamp]\n",
    "            tfrecord_writer.write(serialize_example(feature_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total 10032\ndrwxr-xr-x 2 jarekk jarekk     4096 May 20 19:27 .\ndrwxr-xr-x 5 jarekk jarekk     4096 May 20 19:27 ..\n-rw-r--r-- 1 jarekk jarekk 10261918 May 20 19:27 log_records.tfrecords\n"
    }
   ],
   "source": [
    "!ls -la {local_workspace}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a slicing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/stats_util.py:227: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/stats_util.py:227: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\n"
    }
   ],
   "source": [
    "slice_fn = slicing_util.get_feature_value_slicer(features={'time_window': None})\n",
    "stats_options = tfdv.StatsOptions(\n",
    "    slice_functions=[slice_fn]\n",
    ")\n",
    "\n",
    "stats = tfdv.generate_statistics_from_tfrecord(\n",
    "    data_location=local_tfrecords_file,\n",
    "    stats_options=stats_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "All Examples\ntime_window_2020-05-20\ntime_window_2020-05-19\ntime_window_2020-05-17\ntime_window_2020-05-18\ntime_window_2020-05-16\ntime_window_2020-05-15\n"
    }
   ],
   "source": [
    "for dataset in stats.datasets:\n",
    "    print(dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "lhs_statistics proto contains multiple datasets. Only one dataset is currently supported.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8a1cf96d1b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/display_util.py\u001b[0m in \u001b[0;36mvisualize_statistics\u001b[0;34m(lhs_statistics, rhs_statistics, lhs_name, rhs_name)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0monly\u001b[0m \u001b[0mone\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \"\"\"\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_statistics_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/display_util.py\u001b[0m in \u001b[0;36mget_statistics_html\u001b[0;34m(lhs_statistics, rhs_statistics, lhs_name, rhs_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_statistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     raise ValueError('lhs_statistics proto contains multiple datasets. Only '\n\u001b[0m\u001b[1;32m    203\u001b[0m                      'one dataset is currently supported.')\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: lhs_statistics proto contains multiple datasets. Only one dataset is currently supported."
     ]
    }
   ],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dict = json.loads(raw_data[0])\n",
    "feature_dict = raw_data_dict['instances'][0]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "statistics_proto_list = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "for folder in tf.io.gfile.listdir(drift_reports_path):\n",
    "    statistics_proto = tfdv.load_statistics(\n",
    "        '{}/{}{}'.format(drift_reports_path, folder, 'stats.pb'))\n",
    "    new_stats_proto = statistics_proto_list.datasets.add()\n",
    "    new_stats_proto.CopyFrom(statistics_proto.datasets[0])\n",
    "    new_stats_proto.name = folder[:-1]\n",
    "\n",
    "tfdv.write_stats_text(statistics_proto_list, aggregated_statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsSeries():\n",
    "    def __init__(self, stats_proto_list):\n",
    "        self._stats_proto_list = stats_proto_list\n",
    "\n",
    "    def get_feature_means(self, feature_index):\n",
    "        list_of_means = {\n",
    "             dataset.name[0:16]: dataset.features[feature_index].num_stats.mean\n",
    "             for dataset in self._stats_proto_list.datasets}\n",
    "        \n",
    "        return list_of_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_statistics = tfdv.load_stats_text(aggregated_statistics_path)\n",
    "stats_series = StatsSeries(aggregated_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mean_series = stats_series.get_feature_means(0)\n",
    "feature_mean_series.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "schema_path = 'schema.pbtxt'\n",
    "schema = tfdv.load_schema_text(schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.barplot(x=list(feature_mean_series.keys()), y=list(feature_mean_series.values()), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'gs://mlops-dev-workspace/drift_monitor/output/tf'\n",
    " \n",
    "_STATS_FILENAME='stats.pb'\n",
    "_ANOMALIES_FILENAME='anomalies.pbtxt'\n",
    "\n",
    "stats_output_path = os.path.join(OUTPUT_PATH, _STATS_FILENAME)\n",
    "anomalies_output_path = os.path.join(OUTPUT_PATH, _ANOMALIES_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.load_statistics(stats_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.load_anomalies_text(anomalies_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}